{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - Term frequency - Inverse Document Frequency, i.e. tf-idf = tf * idf ##\n",
    "TF-IDF enables us to gives us a way to associate each word in a document with a number that represents how relevant each word is in that document.\n",
    "`\n",
    "t is the term/ word\n",
    "d is the document\n",
    "D is the total number of documents\n",
    "{ d ∈ D : t ∈ d } denotes the number of documents in which t occur\n",
    "\n",
    "tf-idf = tf * idf\n",
    "\n",
    "Term Frequency = count(t, d) i.e count of term t in document d\n",
    "Normalized term frequency = count(t,d)/Total terms in that document.\n",
    "Logarithmic Term Frequency = 1 + log10(count(t,d))\n",
    "idf ( t, d ) = log ( D / { d ∈ D : t ∈ d })\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\yadava\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-276ae4579060>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\yadava\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import pandas as pd #pandas to deal with tabular data\n",
    "import numpy as np #numpy for number crunching\n",
    "from sklearn import metrics #sklearn provides different ml models & methods to prepare training and test data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import joblib \n",
    "\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    film absolut aw nevertheless hilari time altho...\n",
      "1    well sinc see part s honestli sai never made p...\n",
      "2    got see film preview dazzl it typic romant com...\n",
      "3    adapt posit butcher classic belov subtleti tim...\n",
      "4    zone aw movi simpl seem tri make movi show ree...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load data from csv files\n",
    "# Dataset used ImDb Movie Reviews Dataset https://www.kaggle.com/mantri7/imdb-movie-reviews-dataset?select=train_data+%281%29.csv\n",
    "# Data shape \n",
    "# ['user-review', 'label(0-bad, 1-good)']\n",
    "train_data_from_csv = pd.read_csv('train_data.csv')\n",
    "test_data_from_csv = pd.read_csv('test_data.csv')\n",
    "\n",
    "\n",
    "X_train = train_data_from_csv['0']\n",
    "y_train = train_data_from_csv['1']\n",
    "X_test = test_data_from_csv['0']\n",
    "y_test = test_data_from_csv['1']\n",
    "\n",
    "# preprocess the data\n",
    "#convert to lower\n",
    "X_train = X_train.apply(lambda review: review.lower())\n",
    "X_test = X_test.apply(lambda review: review.lower()) \n",
    "# Removing non ASCII chars\n",
    "X_train = X_train.apply(lambda review: re.sub(r'[^\\x00-\\x7f]',r' ',review))\n",
    "X_test = X_test.apply(lambda review: re.sub(r'[^\\x00-\\x7f]',r' ',review))\n",
    "# Strip multiple whitespaces\n",
    "X_train = X_train.apply(lambda review: gensim.corpora.textcorpus.strip_multiple_whitespaces(review))\n",
    "X_test = X_test.apply(lambda review: gensim.corpora.textcorpus.strip_multiple_whitespaces(review))\n",
    "# Remove all the stopwords\n",
    "X_train = X_train.apply(lambda review: \" \".join([word for word in review.split() if word not in stops]))\n",
    "X_test = X_test.apply(lambda review: \" \".join([word for word in review.split() if word not in stops]))\n",
    "# Removing all the tokens with lesser than 2 characters\n",
    "X_train = X_train.apply(lambda review: \" \".join(gensim.corpora.textcorpus.remove_short([word for word in review.split() if word not in stops], minsize=2)))\n",
    "X_test = X_test.apply(lambda review: \" \".join(gensim.corpora.textcorpus.remove_short([word for word in review.split() if word not in stops], minsize=2)))\n",
    "# Remove the punctuation\n",
    "X_train = X_train.apply(lambda review: gensim.parsing.preprocessing.strip_punctuation2(review))\n",
    "X_test = X_test.apply(lambda review: gensim.parsing.preprocessing.strip_punctuation2(review))\n",
    "# Strip all the numerics\n",
    "X_train = X_train.apply(lambda review: gensim.parsing.preprocessing.strip_numeric(review))\n",
    "X_test = X_test.apply(lambda review: gensim.parsing.preprocessing.strip_numeric(review))\n",
    "# Stemming\n",
    "X_train = X_train.apply(lambda review: gensim.parsing.preprocessing.stem_text(review))\n",
    "X_test = X_test.apply(lambda review: gensim.parsing.preprocessing.stem_text(review))\n",
    "print(X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting words as features from the training and testing sets and making corresponding feature matrices\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words=\"english\", max_features=1200)\n",
    "# extract all the unique words and transform is to make term frequency matrix\n",
    "# we can fit and then trasform but using fit_transform we can do both the steps in single statement\n",
    "# fit is to extract all the unique words i.e vocabulary\n",
    "# transform is to make term frequency matrix of the data for all the unique terms extracted from fit part\n",
    "X_train_tf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# transform the test data into TF vectorized matrix note dont do fit on X_test again because we dont want do create a new vocabulary instead use\n",
    "# the same vocabulary we extracted from training data\n",
    "X_test_tf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build svm classification model\n",
    "svmclf = svm.SVC()\n",
    "# train model \n",
    "svmclf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "accuracy:   0.863\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bad       0.87      0.85      0.86     12500\n",
      "        Good       0.86      0.87      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n",
      "------------------------------\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# predict the output from testing data(unseen data)\n",
    "y_pred = svmclf.predict(X_test_tf)\n",
    "# find the accuracy of the model\n",
    "score = metrics.accuracy_score(y_test, y_pred)\n",
    "print('------------------------------')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print('------------------------------')\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['Bad', 'Good']))\n",
    "print('------------------------------')\n",
    "print(svmclf.predict(tfidf_vectorizer.transform([\"good\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\yadava\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-36d1309c91ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvmclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'svm_clf.joblib'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\yadava\\Miniconda3\\lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# save model to file\n",
    "joblib.dump(svmclf, 'svm_clf.joblib')\n",
    "# load model from file\n",
    "model_loded_from_file = joblib.load('svm_clf.joblib')\n",
    "print(model_loded_from_file.predict(tfidf_vectorizer.transform([\"good\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
